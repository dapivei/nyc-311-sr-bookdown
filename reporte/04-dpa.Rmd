# Infraestructura y Governanza de datos {#dpa}

**Gráfica 2.Data Product Pipeline: Deployment**

<div align="center">

<image width="700" height="100" src="./images/deployment.png">

</div>  

**Gráfica 3.Data Product Pipeline: Desarrollo y Producción**

<div align="center">

<image width="700" height="100" src="./images/desarrollo_produccion.png">

</div>  

## Extract, Transform and Load (ETL)
### Deploy
1. Script en python que hace peticiones a la API de **socrata** usando la librería **sodapy** solicitando todos los registros de la base de datos en formato JSON.
2. En la EC2 se recibe el archivo de datos y se guarda en la instancia S3. El script hace las predicciones por fecha y los archivos respuesta se van guardando en una estructura de carpetas dentro de S3. El S3 está encriptado de tal manera que solamente pueden ingresar con las credenciales asignadas.
3. Alimentamos el esquema **preprocessed** en el cual se genera la misma estructura de carpetas y los archivos JSON se guardan en formato parquet.
4. Se genera el esquema **cleaned** a partir del esquema preprocessed. En este esquema se eliminan las columnas que no tienen variabilidad o son en su mayoría valores nulos. Se asignan los tipos de dato a cada columna y se limpian acentos y caracteres extraños de nombres de columnas y observaciones. Se conserva el formato parquet.


Los detalles de la configuración y uso de la arquitura utilizada en AWS se encuenta en [scripts aws](https://github.com/dapivei/data-product-architecture-final-project/tree/master/scripts/conf-AWS "aws configuración")

### Desarrollo
#### Extract
1. Hacemos una petición a la API solicitando los datos en formato JSON por medio de un script en python. Se hará un petición al día en la que filtramos para incluir solamente los registros que se crearon el día anterior (registros nuevos). La petición actualmente se hace por número de registros. Se modificará para realizar un filtrado utilizando la variable **created_date**.
2. Hacemos una segunda petición a la API solicitando los registros que se cerraron el día anterior en formato JSON. Filtramos la petición por medio de las variables **closed_date** (actualización de registros existentes).

#### Load
1. La petición de los datos en *extract* nos permite obtener los archivos JSON de la API, uno por cada petición y los registros nuevos se incorporan a la estructura de carpetas en S3 con la fecha de creación del registro (futura estructura). Los registros obtenidos por la variable closed_date se guardan en otra estructura de carpatas generada a partir de las fechas.

#### Transform
1. Se corre un script en python (pyspark) que hace consultas a los datos almacenados en parquet, se limpian los datos, se quitan las columnas nulas o que no tienen variabilidad y incorporan los registros al esquema cleaned de la estructura de carpetas.  
2. Se corre un segundo script en python que transforma el archivo JSON que tiene la actualización de registros existentes a parquet, se lleva al formato cleaned y es guarda en la estructura de carpetas.

**Gráfica 4.Extract, Load & Transform**

<div align="center">

<image width="700" height="500" src="./images/elt_aws.png">

</div>  


## Linaje de Datos: Metadatos y Diseño de Tablas(RDS) para la fase del *Extract, Load and Transform(ETL)*

****

**Objetivo**

Presentación de los esquemas que representan el linaje de datos del ETL del proyecto "311 NYC SR".

****

<div align="justify">


### Raw

> **Descripción:**
**1)** Petición a la [API](https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9) los datos, filtrados por fecha.
**2)** Guardado de los datos en formato original (json) dentro de un bucket en `S3`. Nota: Se requieren todas las variables con las que cuenta la base de datos.  

> **Metadata asociada**:

+ Fecha de ejecución
+ Hora de ejecución
+ Parámetros con los que ejecutaste tu task. Lista en formato JSON.
    + year
    + month
    + day
    + bucket_name
+ Quién ejecutó el task (usuario)
+ Desde donde se ejecutó (ip, instancia EC2)
+ Número de registros ingestados
+ Nombre del archivo generado
+ Ruta de almacentamiento en S3 (incluyendo el bucket)
+ Usuario en BD
+ Variables (en el orden en el que aparecen)
+ Tipos de datos **Hay que sacar todos en tipo texto y luego asignamos tipos**
+ Qué script se ejecutó (tag de github): **este paso no es trivial**.
+ Logs del script
+ Estatus de ejecución

### Preprocessed

>**Descripción**: Cambio del formato original (json) al formato de procesamiento (parquet).

> **Metadata asociada**:

+ Fecha de ejecución
+ Hora de ejecución
+ Quién ejecutó (usuario)
+ Cambio de formato, de JSON a Parquet.
+ Tag de código de github que se ejecutó
+ Dónde se ejecutó
+ Idealmente # de registros modificados
+ Logs del scrpit
+ Estatus de ejecución: Fallido, exitoso, etc.

##### Clean

>**Descripción**: **1)** Limpieza de títulos de las columnas, por ejemplo: pasar los datos a minúsculas y quitar caracteres especiales. **2)** Asignación del tipo de dato a las columnas. **3)** Eliminación de las columnas vacías o sin variabilidad.


>**Metadata asociada**:

+ Fecha de ejecución
+ Quién ejecutó (usuario)
+ Dónde se ejecutó
+ Descripción de transformaciones a cada columna
+ Tag de código de github que se ejecutó
+ Número de registros modificados
+ Logs del script (estatus de ejecución por columna)
+ Estatus de ejecución

##### Semantic

> **Descripción:** Las entidades son las agencias; los eventos son los requerimentos de servicio que se tienen. Haremos la predicción de si al día siguiente (granularidad por definir) el número de eventos será mayor que un límite establecido previamente (promedio o por definir). En este paso se generan nuevas variables relevantes para el análisis.

> **Metadata asociada:**

+ Fecha de ejecución
+ Quién ejecutó (usuario)
+ Dónde se ejecutó
+ Nombre de archivos creados
+ Ruta donde se guardaron
+ Descripción de transformaciones a cada columna
+ Descripción de nuevas variables
+ Tag de código de github que se ejecutó
+ Parámetros del script
+ Número de registros modificados
+ Logs del script (estatus de ejecución por columna)
+ Estatus de ejecución

### ML preprocessing

Se le hacen las transformaciónes a los datos con el fin de ponerlos en formato y poderlos procesar con algoritmos de machine learning. Se ponen etiquetas numpericas a las variables categóricas y one hot encoding.


> **Metadata Asociada**:

+ Fecha de ejecución
+ Quién ejecutó (usuario)
+ Dónde se ejecutó
+ Nombre de archivos creados
+ Ruta donde se guardaron
+ Descripción de transformaciones a cada columna
+ Tag de código de github que se ejecutó
+ Parámetros del script
+ Número de registros modificados
+ Logs del script (estatus de ejecución por columna)
+ Estatus de ejecución
